{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehNjBdcxKwCR"
   },
   "source": [
    "# **<center> Análisis de Sentimientos de los candidatos presidenciales </br> Ecuador 2021, en la red social Twitter </center>**\n",
    "<h6>\n",
    "    <center> Miguel Angel Macias, Jonnathan Campoberde <br/> \n",
    "    <i>Universidad de Cuenca <br/>  <i>Facultad de Ingenieria - Escuela de Sistemas<br/> Cuenca, Ecuador</i>\n",
    "    <br/> { mangel.maciasn, jonnathan.campoberde }@ucuenca.edu.ec</center>\n",
    "<h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJY55ow_KwCV"
   },
   "source": [
    "# **I.&emsp;Introducción**\n",
    "El presente trabajo tiene como objetivo emplear de manera práctica todos los conocimientos adquiridos en el transcurso de la asignatura “Text Mining”. Se trabajará con un dataset... . Se busca analizar el ..., detectando las ... por medio de ... . Todo esto con el propósito de ... .\n",
    "\n",
    "## **1.1.&emsp;Objetivos del Proyecto**\n",
    ">El objetivo del proyecto es el Análisis de Sentimientos de los candidatos presidenciales de Ecuador 2021, para luego contrastar los resultados obtenidos con los resultados oficiales de las elecciones Posteriormente se desea responder a las preguntas definidas en 1.2.\n",
    "\n",
    "## **1.2.&emsp;Preguntas de investigación**\n",
    ">- RQ1:     ¿Qué tan precisamente los datos de Twitter reflejan el sentimiento político en un proceso electoral?\n",
    ">- RQ1:     ¿En qué grado de aceptación se encuentran los candidatos dentro de la red Social Twitter?\n",
    ">- RQ3:     ¿Los resultadso oficiales son parecidos a los hallados en nuestro análisis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II.&emsp;TEXT CLEANING**\n",
    "\n",
    "## **2.1.&emsp; Reading dataset**\n",
    ">Para este paso..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#from datetime import datetime, timezone\n",
    "import pytz\n",
    "import numpy as np              # Para operaciones matematicas\n",
    "import pandas as pd             # Manejo de datos / dataframes\n",
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = '../data/raw'              # directorio que contiene toda la data sin manipular\n",
    "data_processed = '../data/processed'  # directorio que contiene data procesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arauz = pd.read_csv(data_raw+\"/all_tweets_arauz.csv\")\n",
    "df_arauz['created_at'] = pd.to_datetime(df_arauz['created_at'], utc=pytz.UTC)\n",
    "df_arauz['user.created_at'] = pd.to_datetime(df_arauz['user.created_at'], utc=pytz.UTC)\n",
    "df_arauz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2.&emsp; Removing Duplicates**\n",
    ">lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "len(df_arauz['full_text'])-len(df_arauz['full_text'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arauz_processed = df_arauz.drop_duplicates( ['full_text'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_arauz_processed['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3.&emsp; Identify tweets by trolls**\n",
    ">Lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_troll(Dt, Dc, Fc, fc, v):\n",
    "    '''\n",
    "    ----------------------------------\n",
    "    | Variable Description           |\n",
    "    ----------------------------------\n",
    "    | Dt  stands for Date Tweet      |\n",
    "    | Dc  stands for Date creation   |\n",
    "    | Fc  stands for Followers count |\n",
    "    | fc  stands for friend count    |\n",
    "    | v   stands for verified        |\n",
    "    ----------------------------------\n",
    "    | Weight penalization            |\n",
    "    ----------------------------------\n",
    "    | Timeliness metric:  WD = 0.7   |\n",
    "    | Followers metric:   WF = 0.2   |\n",
    "    | Following metric:   Wf = 0.1   |\n",
    "    ----------------------------------\n",
    "    | Acceptance Threshold           |\n",
    "    ----------------------------------\n",
    "    | Non Troll   > 0.8              |\n",
    "    | Troll       < 0.8              |\n",
    "    ----------------------------------\n",
    "    '''\n",
    "    # If user is verified then is a real person\n",
    "    if v:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        threshold = 0.7*((Dt-Dc).days)/180 + 0.2*(Fc/250) + 0.1*(fc/300)\n",
    "\n",
    "        return threshold < 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arauz_processed['is_troll'] = df_arauz_processed.apply(lambda x: is_troll(x['created_at'],\n",
    "                                                         x['user.created_at'],\n",
    "                                                         x['user.followers_count'],\n",
    "                                                         x['user.friends_count'],\n",
    "                                                         x['user.verified']), axis=1)\n",
    "# Se excluyen tweets trolls del dataset\n",
    "#df_arauz_clean = df_arauz_processed[df_arauz_processed['is_troll']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4.&emsp; Removing characters and Specific Stopwords**\n",
    ">lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Function to clean text extracted from the Internet with html tags\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str \n",
    "      original string\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    text: str\n",
    "      modified initial string\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text) #Remove http links\n",
    "    text = EMOJI.sub(r'', text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtrado de caracteres y stopwords\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@#:,;¿?!¡-]')\n",
    "EMOJI = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "STOPWORDS = ('rt')  # Adding new stopwords to the set, 'cause theese words repeat in all docs\n",
    "\n",
    "df_arauz_processed['full_text'] = df_arauz_processed[\"full_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Luego de remover los caracteres especiales pueden existir Tweets vacios, debido a que estos solo contenian emotes o enlaces en su interior. Es por esta razon que se buscan estos espacios en blanco y se los quita_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arauz_processed[df_arauz_processed['full_text']==\"\"][\"full_text\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arauz_processed.drop(df_arauz_processed.loc[df_arauz_processed[\"full_text\"]==\"\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arauz_processed[df_arauz_processed['full_text']==\"\"][\"full_text\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arauz_processed.to_csv(data_processed+'/arauz_clean.csv', \n",
    "                        index=False, header=True, chunksize=10000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Trabajo Final Data Science G4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "darlin",
   "language": "python",
   "name": "darlin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
